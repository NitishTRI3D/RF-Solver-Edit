Real Doubts:

why is opts.seed = None made after generating something randomly 

there is no feature_path , what is the used for ?



Understaning the steps in second order solver, denoise


------------------------------------------------------------------------------------------------------------------------------------------
To be printed and clarified :

print time_steps and check what that is , will time_steps change so much because of length of inp["img"] and inp_targer["img"]

print things in timeshift - seems to be doing a lot in dev model 

go through forward function clearly and start printing things 

(1024, 1024, 3) uint8 init_image initially
torch.Size([3, 1024, 1024]) torch.float32 init_image after permute
torch.Size([1, 3, 1024, 1024]) torch.float32 init_image after unsqueeze
torch.Size([1, 3, 1024, 1024]) torch.float32 init_image after to
torch.Size([1, 16, 128, 128]) torch.bfloat16 init_image after encode
------------------------------------------------------------------------------------------------------------------------------------------
PARAMETERS TO PLAY 
- shifts 
- guidance 
- inject 
- block division 
- Using custom models which are more text aligned to what we want to achieve 


beyond guidance_vec and inject_steps , how the image_ids are blocked could be useful, features like thin, fat cannot be blocked 
but aeroplane, dress etc can be limited to a region - hopefully there is a play there 


------------------------------------------------------------------------------------------------------------------------------------------

NOTES:
is info passed in regular FLux model or this was added by these guys ? - added by these guys 


------------------------------------------------------------------------------------------------------------------------------------------
Understand flux code base for image to image

What is the syntax ** z, info = denoise(model, **inp, timesteps=timesteps, guidance=1, inverse=True, info=info)

offload by default is True , we can try False as we have 80GB in our cards 

------------------------------------------------------------------------------------------------------------------------------------------


print guidance_vec , its more like a vector with same value that's used like a strength I guess 


parser.add_argument('--inject', type=int, default=20,
                        help='the number of timesteps which apply the feature sharing')
understand the inject thing in detail 


Check if torch.set_grad_enabled(False) is set to True only while training 


ModelSpec looks deadly simple 
"flux-dev": ModelSpec(
        repo_id="black-forest-labs/FLUX.1-dev",
        repo_flow="flux1-dev.safetensors",
        repo_ae="ae.safetensors",
        ckpt_path=os.getenv("FLUX_DEV"),
        params=FluxParams(
            in_channels=64,
            vec_in_dim=768,
            context_in_dim=4096,
            hidden_size=3072,
            mlp_ratio=4.0,
            num_heads=24,
            depth=19,
            depth_single_blocks=38,
            axes_dim=[16, 56, 56],
            theta=10_000,
            qkv_bias=True,
            guidance_embed=True,
        ),
        ae_path=os.getenv("AE"),
        ae_params=AutoEncoderParams(
            resolution=256,
            in_channels=3,
            ch=128,
            out_ch=3,
            ch_mult=[1, 2, 4, 4],
            num_res_blocks=2,
            z_channels=16,
            scale_factor=0.3611,
            shift_factor=0.1159,
        ),
    ),


Check if the code works without api.py or check where it is used

get rid of watermarking 

loop and parse_prompts does not look functional

What is the difference between unpack and ae.decode again?


